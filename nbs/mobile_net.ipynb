{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd \n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Ignore warnings \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "# Interactive mode on \n",
    "plt.ion() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device: 0\n",
      "Current Device Name: TITAN V\n",
      "Current Device Specs: _CudaDeviceProperties(name='TITAN V', major=7, minor=0, total_memory=12066MB, multi_processor_count=80)\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "\n",
    "cuda.set_device(0)\n",
    "current_dev = cuda.current_device()\n",
    "current_dev_name = cuda.get_device_name(current_dev)\n",
    "current_dev_specs = cuda.get_device_properties(current_dev)\n",
    "\n",
    "print(f'Current Device: {current_dev}')\n",
    "print(f'Current Device Name: {current_dev_name}')\n",
    "print(f'Current Device Specs: {current_dev_specs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/SGF.EDUBEAR.NET/eam96/.cache/torch/hub/pytorch_vision_v0.4.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNReLU(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNReLU(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenet = torch.hub.load('pytorch/vision:v0.4.2', 'mobilenet_v2', pretrained=True)\n",
    "mobilenet.classifier[1] = nn.Linear(1280, 2)\n",
    "mobilenet.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for model\n",
    "path = Path.cwd().parents[0] / 'data' / 'heat_maps'\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "img_set = ImageFolder(path, transform=preprocess)\n",
    "trainloader = DataLoader(img_set, batch_size=4, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNet hyperparameters \n",
    "num_epochs = 1\n",
    "eta = 0.001\n",
    "nu = 0.9\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet.parameters(), lr=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.335\n",
      "[1,    20] loss: 0.172\n",
      "[1,    30] loss: 0.019\n",
      "[1,    40] loss: 0.003\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to('cuda'), data[1].to('cuda')\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = mobilenet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for model\n",
    "path = Path.cwd().parents[0] / 'data' / 'heat_maps'\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "img_set = ImageFolder(path, transform=preprocess)\n",
    "testloader = DataLoader(img_set, batch_size=len(img_set), num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Accuracy on test data: 100.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to('cuda'), data[1].to('cuda') \n",
    "\n",
    "        outputs = mobilenet(inputs)\n",
    "\n",
    "        correct, total = 0, outputs.shape[0]\n",
    "        for output, label in zip(outputs, labels):\n",
    "            hypothesis = output.max(0)[1].item()\n",
    "            print(f'Output: {hypothesis} \\tExpected: {label}')\n",
    "            \n",
    "            if hypothesis == label: correct += 1\n",
    "                \n",
    "        print(f'Accuracy on test data: {(correct/total)*100}')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/SGF.EDUBEAR.NET/eam96/Net_Sentinel/nbs\n"
     ]
    }
   ],
   "source": [
    "torch.save(mobilenet.state_dict(), '/home/SGF.EDUBEAR.NET/eam96/Net_Sentinel/data/models/mobile_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/SGF.EDUBEAR.NET/eam96/.cache/torch/hub/pytorch_vision_v0.4.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (6): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squeezenet = torch.hub.load('pytorch/vision:v0.4.2', 'squeezenet1_1', pretrained=True)\n",
    "squeezenet.classifier[1] = nn.Conv2d(512, 2, kernel_size=(1,1), stride=(1,1))\n",
    "squeezenet.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNet hyperparameters \n",
    "num_epochs = 1\n",
    "eta = 0.001\n",
    "nu = 0.9\n",
    "\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = optim.Adam(squeezenet.parameters(), lr=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.993\n",
      "[1,    20] loss: 0.329\n",
      "[1,    30] loss: 0.050\n",
      "[1,    40] loss: 0.006\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to('cuda'), data[1].to('cuda')\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = squeezenet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Output: 0 \tExpected: 0\n",
      "Output: 1 \tExpected: 1\n",
      "Accuracy on test data: 100.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to('cuda'), data[1].to('cuda') \n",
    "\n",
    "        outputs = squeezenet(inputs)\n",
    "\n",
    "        correct, total = 0, outputs.shape[0]\n",
    "        for output, label in zip(outputs, labels):\n",
    "            hypothesis = output.max(0)[1].item()\n",
    "            print(f'Output: {hypothesis} \\tExpected: {label}')\n",
    "            \n",
    "            if hypothesis == label: correct += 1\n",
    "                \n",
    "        print(f'Accuracy on test data: {(correct/total)*100}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(squeezenet.state_dict(), '/home/SGF.EDUBEAR.NET/eam96/Net_Sentinel/data/models/squeeze_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
